{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "похожи на ndarrays (на массивы и матрицы, \n",
    "но тензоры могут работать на графических процессорах и др апп. ускорителях), <br>\n",
    "тензоры используются для кодирования входных и выходных данных модели и ее параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialization of tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.2913, 0.8471],\n",
      "        [0.9278, 0.7325]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Directly from data\n",
    "\n",
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "# From numpy array\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "\n",
    "# From another tensor\n",
    "\n",
    "# Новый тензор сохраняет свойства (форму, тип данных) тензора аргумента,\n",
    "# если он явно не переопределен\n",
    "x_ones = torch.ones_like(x_data) #retains the properties of x_data\n",
    "print(f'Ones Tensor: \\n {x_ones} \\n')\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) #overrides the datatype pf x_data\n",
    "print(f'Random Tensor: \\n {x_rand} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.7744, 0.5244, 0.7994],\n",
      "        [0.6083, 0.5997, 0.0960],\n",
      "        [0.0784, 0.5947, 0.4419]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# With random or const (zero/ones) values\n",
    "shape=(3,3,)\n",
    "\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributes of a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor attribute describe their shape, datatype, device they are stored on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "tensor.shape, tensor.dtype, tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переместить наш тензор в GPU если он доступен\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor API similar to NumPy API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row tensor([0.2447, 0.0098, 0.8712, 0.4402])\n",
      "First column tensor([0.2447, 0.2698, 0.0721, 0.3200])\n",
      "Last column tensor([0.4402, 0.7655, 0.3426, 0.9784])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(4,4)\n",
    "\n",
    "print('First row', tensor[0])\n",
    "print('First column', tensor[:, 0])\n",
    "print('Last column', tensor[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2447, 0.0098, 0.8712, 0.4402, 0.2447, 0.0098, 0.8712, 0.4402],\n",
      "        [0.2698, 0.6334, 0.1269, 0.7655, 0.2698, 0.6334, 0.1269, 0.7655],\n",
      "        [0.0721, 0.0388, 0.2950, 0.3426, 0.0721, 0.0388, 0.2950, 0.3426],\n",
      "        [0.3200, 0.1868, 0.0192, 0.9784, 0.3200, 0.1868, 0.0192, 0.9784]])\n",
      "tensor([[[0.2447, 0.0098, 0.8712, 0.4402],\n",
      "         [0.2698, 0.6334, 0.1269, 0.7655],\n",
      "         [0.0721, 0.0388, 0.2950, 0.3426],\n",
      "         [0.3200, 0.1868, 0.0192, 0.9784]],\n",
      "\n",
      "        [[0.2447, 0.0098, 0.8712, 0.4402],\n",
      "         [0.2698, 0.6334, 0.1269, 0.7655],\n",
      "         [0.0721, 0.0388, 0.2950, 0.3426],\n",
      "         [0.3200, 0.1868, 0.0192, 0.9784]]])\n"
     ]
    }
   ],
   "source": [
    "# Joining tensors\n",
    "\n",
    "t1 = torch.cat([tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.stack([tensor, tensor], dim=0)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Арифметические операции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0127, 0.5197, 0.4258, 0.5275],\n",
      "        [0.5197, 1.0760, 0.3437, 0.9560],\n",
      "        [0.4258, 0.3437, 0.2111, 0.3711],\n",
      "        [0.5275, 0.9560, 0.3711, 1.0948]]) \n",
      " tensor([[1.0127, 0.5197, 0.4258, 0.5275],\n",
      "        [0.5197, 1.0760, 0.3437, 0.9560],\n",
      "        [0.4258, 0.3437, 0.2111, 0.3711],\n",
      "        [0.5275, 0.9560, 0.3711, 1.0948]]) \n",
      " tensor([[1.0127, 0.5197, 0.4258, 0.5275],\n",
      "        [0.5197, 1.0760, 0.3437, 0.9560],\n",
      "        [0.4258, 0.3437, 0.2111, 0.3711],\n",
      "        [0.5275, 0.9560, 0.3711, 1.0948]])\n",
      "tensor([[5.9883e-02, 9.6296e-05, 7.5896e-01, 1.9374e-01],\n",
      "        [7.2791e-02, 4.0115e-01, 1.6104e-02, 5.8594e-01],\n",
      "        [5.1931e-03, 1.5046e-03, 8.7029e-02, 1.1737e-01],\n",
      "        [1.0239e-01, 3.4889e-02, 3.6726e-04, 9.5720e-01]]) \n",
      " tensor([[5.9883e-02, 9.6296e-05, 7.5896e-01, 1.9374e-01],\n",
      "        [7.2791e-02, 4.0115e-01, 1.6104e-02, 5.8594e-01],\n",
      "        [5.1931e-03, 1.5046e-03, 8.7029e-02, 1.1737e-01],\n",
      "        [1.0239e-01, 3.4889e-02, 3.6726e-04, 9.5720e-01]]) \n",
      " tensor([[5.9883e-02, 9.6296e-05, 7.5896e-01, 1.9374e-01],\n",
      "        [7.2791e-02, 4.0115e-01, 1.6104e-02, 5.8594e-01],\n",
      "        [5.1931e-03, 1.5046e-03, 8.7029e-02, 1.1737e-01],\n",
      "        [1.0239e-01, 3.4889e-02, 3.6726e-04, 9.5720e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "y1 = tensor @ tensor.T\n",
    "\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out = y3)\n",
    "\n",
    "print(y1, '\\n', y2,'\\n', y3)\n",
    "\n",
    "# Computes element-wise product \n",
    "z1 = tensor*tensor\n",
    "\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "\n",
    "print(z1,'\\n', z2,'\\n', z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.614131927490234 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# агрегирование всех значений тензора в одно числовое значение Python (item())\n",
    "\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.2447, 5.0098, 5.8712, 5.4402],\n",
       "        [5.2698, 5.6334, 5.1269, 5.7655],\n",
       "        [5.0721, 5.0388, 5.2950, 5.3426],\n",
       "        [5.3200, 5.1868, 5.0192, 5.9784]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In place operations (store the result into the operand) has _\n",
    "\n",
    "tensor.add_(5)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.2447, 5.2698, 5.0721, 5.3200],\n",
       "        [5.0098, 5.6334, 5.0388, 5.1868],\n",
       "        [5.8712, 5.1269, 5.2950, 5.0192],\n",
       "        [5.4402, 5.7655, 5.3426, 5.9784]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.t_()\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two data primitives: **torch.utils.data.DataLoader** and **torch.utils.data.Dataset** <br>\n",
    "\n",
    "**Dataset** stores the samples and their corresponding labels, and \n",
    "**DataLoader** wraps an iterable around the Dataset to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FashionMNIST example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.FashionMNIST(root = 'data', #path where the train/test data is stored,\n",
    "                             train=True, #specifies training or test dataset\n",
    "                             download=True, #downloads the data from the internet if it’s not available at root\n",
    "                             transform=ToTensor())\n",
    "\n",
    "test = datasets.FashionMNIST(root='data',\n",
    "                        train=False,\n",
    "                        download=True,\n",
    "                        transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating and visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: 'T-Shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot',\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols + rows + 1):\n",
    "    sample_idx = torch.randint(len(train), size=(1,)).item()\n",
    "    img, label = train[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.squeeze(), cmap='jet')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a custom Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDateset(Dataset):\n",
    " \n",
    "# __init__ запускается один раз при создании экземпляра объекта Dataset \n",
    "# Мы инициализируем каталог, содержащий изображения, файл аннотаций и оба преобразования \n",
    "\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "# Функция __len__ возвращает количество выборок в нашем наборе данных.       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "#Функция __getitem__ загружает и возвращает образец из набора данных по заданному индексу idx\n",
    "#На основе индекса он определяет расположение изображения на диске, преобразует его в тензор с помощью read_image, \n",
    "#извлекает соответствующую метку из данных csv в self.img_labels, \n",
    "#вызывает для них функции преобразования (если применимо) и возвращает тензорное изображение и соответствующую метку в кортеж\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.ing_labels.iloc[idx,1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform([label])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for training with DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Dataset** retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s **multiprocessing** to speed up data retrieval.<br>\n",
    "\n",
    "**DataLoader** is an iterable that abstracts this complexity for us in an easy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPXklEQVR4nO3de4xc5XnH8d+vlEBsroZAjO2GJHXaoCQl6YpWstU4pURgRVyUC7htZCpSRy2RiIpUECB5qdTIRXXSKKpoN4XipJQkUqBYLUK4TgICiZQ1dTHESc3FgcXGDkbBDsaA4ekfO6QL7Lzv7py5wfP9SKuZnedcHs/uz2d23nPmdUQIwFvfrwy6AQD9QdiBJAg7kARhB5Ig7EASv9rPndlzQjqmn7t8S/jQb+8u1g/9+ctta88dc3hx3ac0v1h/UW8r1t+u54v1w/RC29r8p3YV1937ziOK9W2bjizWc/q5IvZ7uoqbDL3ZPlPSVyUdIumfImJNefmTQlrV8f6ymoivFesL1j/Ttnbf2R8orvs3uqxYf0wnF+sf1JZi/df1SNvaVdesLa57+19+tFg/y8uK9ZzGFLFj2rB3/DLe9iGS/l7SWZJOkbTC9imdbg9AbzX5m/00SQ9HxKMR8aKkb0k6pzttAei2JmFfIOmJKd9PtB57DdurbI/bHpf2N9gdgCaahH26vwve8AZARIxFxEhEjEhzGuwOQBNNwj4hadGU7xdK2tGsHQC90iTs90labPvdtt8m6QJJ67vTFoBuazr0tlzS32ly6O36iPjr8vIMvXUiVlxdXmBPofZsZeOV+o9+XK7/tLL50kj4nZV1l1TqH9PqyhIZtR96a3RSTUTcJum2JtsA0B+cLgskQdiBJAg7kARhB5Ig7EAShB1Ioq/Xs2N6C+OPivUJl8fZF/5HoVj7CZ9QLp/yYKW+rbL9wjj+0soY/+gNlW3fPlqun1mpJ8ORHUiCsANJEHYgCcIOJEHYgSQIO5AEQ29D4Imz31de4MLKBg52WJuJ2iWyWxtsu/Lbd3Jl9XjXtFdy/pK5BPY1OLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMsw+D2mWip1fqpZ9ibZy9/YzKkw5rsG9JOlCpF5xVW+DmzredEUd2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfZhsKJS312p/1qhVhsn31Wp18bh51XqjxdqlY+xnqhs+uhLKgtcWakn0yjstrdL2ifpZUkHI2KkG00B6L5uHNk/FhFPd2E7AHqIv9mBJJqGPSTdYXuT7VXTLWB7le1x2+PS/oa7A9Cppi/jl0TEDtsnSNpg+8cRcdfUBSJiTNKYJNknRcP9AehQoyN7ROxo3e6WdIuk07rRFIDu6zjstufaPvLV+5I+Lqky5yeAQWnyMv5ESbfYfnU7/xoRt3elq2zGK/WllXrps90rY9m6q1K/t1K/sFJ/plIv2F6pHz/3HZ1vPKGOwx4Rj0r6rS72AqCHGHoDkiDsQBKEHUiCsANJEHYgCS5x7YsFxereO8prH3V2ZfOln2LlzIcb1pbrJ1d2veziygKlS2CfK69aG7V7QosqS2AqjuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7H1xfrG654VpP9Hrl446rrL5Le1Le79UXnVJZdMnHagcD5a/Uq6XzhGoDKTXxtk3iQ8zng2O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPs/XDMUcXyUfsq6++o1J9oX9pQmXL5k/eV6/7Ey8X62u+5WP+LzxSKh5T3fbBc1k/0G5Ul9lbquXBkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGfvhwvK5b3/UK4fd6Cy/cJnsxcudZckfWr+8+UF/nO0WH6psv3iWPrhtZXLtuiDlSXuabaDt5jqkd329bZ3235wymPzbG+wva11e2xv2wTQ1Exext8g6czXPXa5pI0RsVjSxtb3AIZYNewRcZfe+AlB50ha17q/TtK5Xe4LQJd1+gbdiRGxU5Jatye0W9D2Ktvjtsel/R3uDkBTPX83PiLGImIkIkakOb3eHYA2Og37LtvzJal1u7t7LQHohU7Dvl7Sytb9lZJu7U47AHqlOs5u+yZJyyQdb3tC0mpJayR9x/ZFkh6X9OleNvmmt7RcnlMZZ6/a2mDdhWsa7fojtQVK88O/v7xqbQz/7ofOqCzBOPtU1bBHxIo2pdO73AuAHuJ0WSAJwg4kQdiBJAg7kARhB5LgEtd+2F4uV08irs1d/Fj70qG1bTd0xu9UFujl6VZckTErHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2fuhdJmnZjAWvqdc3rutfe0PK5u+qrbvmrUN6pV/V9XDow03kAtHdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2IfBwpb7whXJ9olC/J/64vLIrO685rlIvXYtf+e2rTciM2eHIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM7eD9vL5V219Ss/pdL18Ku+/c3Kxkdrey+6+zfLkzYvfe7+9sVny9v+5OLKzgvX8eONqkd229fb3m37wSmPjdp+0vbm1tfy3rYJoKmZvIy/QdKZ0zz+lYg4tfV1W3fbAtBt1bBHxF2qT0AEYMg1eYPuC7YfaL3MP7bdQrZX2R63PT6DWc0A9EinYb9W0nslnSpppwofKxgRYxExEhEj0pwOdwegqY7CHhG7IuLliHhF0tclndbdtgB0W0dhtz1/yrfnqfphyQAGrTrObvsmScskHW97QtJqSctsnyopNDmK/Pke9vjmd295QPj52voHy+UnS8XDaxtvZqP+oFhfeqAwzl5727dyKb5WV+p4jWrYI2LFNA9f14NeAPQQp8sCSRB2IAnCDiRB2IEkCDuQBJe49sWNxWr5IlFJz5XLL5WKB2obb2b1tmvKC5R+wyq9vXRJbeeVOl6DIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4+xD40OcqC2wulxcUamvP//PiupdecEJl5xXfq9QPK9Qq5w/82dFfq2z86UodU3FkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkHBH925lPCmlV3/b3ZvFI/GOx/p4TnyrWo/BR0763vG+/r9lF4fH7V5cXKJ3JsaO8qj9R+d1cM1qupzSmiB2ersKRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4Hr2IfBvOq9Y/9yz1xbrR51UKP5LBw3NwrbK9eyLlxeKR1c2vma23aCkemS3vcj2921vtf2Q7Utaj8+zvcH2ttbtsb1vF0CnZvIy/qCkSyPi/ZJ+V9LFtk+RdLmkjRGxWNLG1vcAhlQ17BGxMyLub93fJ2mrJj8J6RxJ61qLrZN0bq+aBNDcrN6gs32ypA9L+qGkEyNipzT5H4KkaT/MzPYq2+O2x6X9zboF0LEZh932EZK+K+mLEbF3putFxFhEjETEiDSnkx4BdMGMwm77UE0G/caIuLn18C7b81v1+ZJ296ZFAN1QHXqzbUnXSdoaEV+eUlovaaUmB0hWSrq1Jx0m8G2dX6yf90Jl6K30U/zvDhqahe2V+uJSsTRkKEkq/7sxOzMZZ18i6bOStth+9RPMr9BkyL9j+yJJj0v6dG9aBNAN1bBHxN2Spr0YXtLp3W0HQK9wuiyQBGEHkiDsQBKEHUiCsANJcInrEPivez5arO+rbaA09fELs+1mdnbVFjhQqJ3aeOuYBY7sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+zD4N/L5XmV1fcUhqOPq63c0JO1BUrj/Hd2sRFUcWQHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZx8G28vlhdNOrPX/9uwp1LbOuptZuax8KX75WvvSdM6SdM8sm0ERR3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSGIm87MvkvQNSe+U9IqksYj4qu1RSX8q6WetRa+IiNt61ehb2r2V+txyed7B9rUvPTPrbl5nWbl84Opy/dn2pTuvPq287l+Vy9LbK/XnaxtIZSYn1RyUdGlE3G/7SEmbbG9o1b4SEX/bu/YAdMtM5mffKWln6/4+21slLeh1YwC6a1Z/s9s+WdKHJf2w9dAXbD9g+3rbx7ZZZ5Xtcdvj0v5GzQLo3IzDbvsISd+V9MWI2CvpWknv1eSMXTslrZ1uvYgYi4iRiBiR5nShZQCdmFHYbR+qyaDfGBE3S1JE7IqIlyPiFUlfl1R5twXAIFXDbtuSrpO0NSK+POXx+VMWO0/Sg91vD0C3zOTd+CWSPitpi+3NrceukLTC9qmSQpMXaX6+Jx1mUJrWWNL+ypzNcwpDc1f+c3ndq/6kXJd+UC7vLpc3Pda+NqfxezgMrc3GTN6Nv1uSpykxpg68iXAGHZAEYQeSIOxAEoQdSIKwA0kQdiAJPkp6GDw1WizPXRbl9X9QWL86jt6MH1tdXuBTo4WVC3NNS5o8IxvdwpEdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JwRGUMt5s7s38m6adTHjpe0tN9a2B2hrW3Ye1LordOdbO3d0XEO6Yr9DXsb9i5PT752XTDZ1h7G9a+JHrrVL9642U8kARhB5IYdNjHBrz/kmHtbVj7kuitU33pbaB/swPon0Ef2QH0CWEHkhhI2G2fafsnth+2ffkgemjH9nbbW2xvnpyfbqC9XG97t+0Hpzw2z/YG29tat9POsTeg3kZtP9l67jbbXj6g3hbZ/r7trbYfsn1J6/GBPneFvvryvPX9b3bbh0j6X0lnSJqQdJ+kFRHxo7420obt7ZJGImLgJ2DY/j1Jv5D0jYj4QOuxayQ9ExFrWv9RHhsRlw1Jb6OSfjHoabxbsxXNnzrNuKRzJV2oAT53hb4+oz48b4M4sp8m6eGIeDQiXpT0LUnnDKCPoRcRd0l65nUPnyNpXev+Ok3+svRdm96GQkTsjIj7W/f3SXp1mvGBPneFvvpiEGFfIOmJKd9PaLjmew9Jd9jeZHvVoJuZxokRsVOa/OWRdMKA+3m96jTe/fS6acaH5rnrZPrzpgYR9ummkhqm8b8lEfERSWdJurj1chUzM6NpvPtlmmnGh0Kn0583NYiwT0haNOX7hZJ2DKCPaUXEjtbtbkm3aPimot716gy6rdvK1Ir9M0zTeE83zbiG4Lkb5PTngwj7fZIW23637bdJukDS+gH08Qa257beOJHtuZI+ruGbinq9pJWt+ysl3TrAXl5jWKbxbjfNuAb83A18+vOI6PuXpOWafEf+EUlXDqKHNn29R9L/tL4eGnRvkm7S5Mu6lzT5iugiScdJ2ihpW+t23hD19k1JWyQ9oMlgzR9Qb0s1+afhA5I2t76WD/q5K/TVl+eN02WBJDiDDkiCsANJEHYgCcIOJEHYgSQIO5AEYQeS+D9F8j/kTUd8PAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "# display image and label\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f'Feature batch shape: {train_features.size()}')\n",
    "print(f'Labels batch shape: {train_labels.size()}')\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "\n",
    "plt.imshow(img, cmap='jet')\n",
    "plt.show();\n",
    "print(f'label: {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for training data<br>\n",
    "***features*** as ***normalized*** tensors, - **ToTensor** <br>\n",
    "***labels*** - as ***OneHotEncoded*** tensors - **Lambda**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All TorchVision datasets have two parameters\n",
    "* **transform** - to modify *features*\n",
    "* **target_transform** - to modify *labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(root='data',\n",
    "                          train=True,\n",
    "                          download=True,\n",
    "                          transform=ToTensor(),\n",
    "                          target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ToTensor()'></a>\n",
    "### ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToTensor** converts a PIL image or NumPy ndarray into a **FloatTensor**. and scales the image's pixel intensity values\n",
    "in the range[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lambda Transforms**\n",
    "<br>\n",
    "apply any user-defined lambda func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(10, #10 - number of labels in our dataset\n",
    "                   dtype=torch.float).scatter_(dim=0, #calls scatter which assigns a value=1\n",
    "                   index=torch.tensor(y),value=1)) #in the index as given by the label y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "### Get Device for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(nn.Linear(28*28, 512),\n",
    "                                              nn.ReLU(),\n",
    "                                              nn.Linear(512, 512),\n",
    "                                              nn.ReLU(),\n",
    "                                              nn.Linear(512, 10))\n",
    "    def forward(self, x):\n",
    "        x=self.flatten(x)\n",
    "        logits=self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created an example of NN and moving it to the **device** and print it's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model we must **pass the input data**<br>\n",
    "*This executes the model’s forward, along with some background operations. Do not call model.forward() directly!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([4])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1,28,28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f'Predicted class: {y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.Linear**<br>\n",
    "The linear layer is a module that applies a linear transformation on the input **using its stored weights and biases.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.ReLU**<br>\n",
    "Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.1008, -0.2885,  0.3573, -0.3053,  0.0100, -0.0300, -0.0553, -0.5678,\n",
      "          0.1561, -0.0092,  0.7603, -0.3161, -0.1854, -0.1598,  0.6945, -0.0409,\n",
      "          0.2833, -0.0873, -0.0921,  0.6589],\n",
      "        [-0.1311, -0.2265,  0.1382, -0.2722, -0.2172, -0.2868,  0.1107, -0.1841,\n",
      "         -0.5107,  0.0837,  0.2691, -0.1622, -0.4034, -0.0136,  0.0426, -0.1035,\n",
      "          0.4145, -0.3742,  0.1324,  0.2496],\n",
      "        [ 0.2344, -0.1922,  0.1264, -0.3666, -0.2629, -0.1084,  0.3915, -0.3547,\n",
      "         -0.2452, -0.1371,  0.7106, -0.1495, -0.0756, -0.4178,  0.3214, -0.0628,\n",
      "          0.3701, -0.0502,  0.0210,  0.2929]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.1008, 0.0000, 0.3573, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 0.1561,\n",
      "         0.0000, 0.7603, 0.0000, 0.0000, 0.0000, 0.6945, 0.0000, 0.2833, 0.0000,\n",
      "         0.0000, 0.6589],\n",
      "        [0.0000, 0.0000, 0.1382, 0.0000, 0.0000, 0.0000, 0.1107, 0.0000, 0.0000,\n",
      "         0.0837, 0.2691, 0.0000, 0.0000, 0.0000, 0.0426, 0.0000, 0.4145, 0.0000,\n",
      "         0.1324, 0.2496],\n",
      "        [0.2344, 0.0000, 0.1264, 0.0000, 0.0000, 0.0000, 0.3915, 0.0000, 0.0000,\n",
      "         0.0000, 0.7106, 0.0000, 0.0000, 0.0000, 0.3214, 0.0000, 0.3701, 0.0000,\n",
      "         0.0210, 0.2929]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.Sequential**<br>\n",
    "nn.Sequential is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like seq_modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.Softmax**<br>\n",
    "The last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the nn.Softmax module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing nn.Module automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s parameters() or named_parameters() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0191,  0.0017,  0.0196,  ...,  0.0346,  0.0311,  0.0006],\n",
      "        [ 0.0157,  0.0277,  0.0327,  ...,  0.0264,  0.0287, -0.0242]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0131,  0.0035], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0262, -0.0410, -0.0044,  ...,  0.0093, -0.0247, -0.0105],\n",
      "        [-0.0433, -0.0415,  0.0210,  ..., -0.0223,  0.0414, -0.0374]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0278,  0.0016], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0353, -0.0094, -0.0130,  ...,  0.0406, -0.0287, -0.0094],\n",
      "        [-0.0424, -0.0113, -0.0109,  ..., -0.0035,  0.0040,  0.0241]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0144, 0.0336], grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this example, we iterate over each parameter, and print its size and a preview of its values.\n",
    "\n",
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "* N of epochs\n",
    "  1. The train loop\n",
    "  2. The validation/ Test loop\n",
    "* Batch size\n",
    "* learninf rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss func\n",
    "* nn.MSELoss - **regression tasks**\n",
    "* nn.NLLLoss (Negative Log Likelihood) - **classification tasks**\n",
    "* nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass our model’s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error.\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "[Torch.Optim](https://pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итогововый код для НС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train_loop that loops over the optimization code,\n",
    "# and test_loop that evaluates the model's performance against our test data\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size= len(dataloader.dataset)\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        \n",
    "        #compute prediction and loss\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred, y)\n",
    "        \n",
    "        #Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size=len(dataloader.dataset)\n",
    "    num_batches=len(dataloader)\n",
    "    test_loss, correct = 0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.288334  [    0/60000]\n",
      "loss: 2.285249  [ 6400/60000]\n",
      "loss: 2.257934  [12800/60000]\n",
      "loss: 2.253871  [19200/60000]\n",
      "loss: 2.238527  [25600/60000]\n",
      "loss: 2.196038  [32000/60000]\n",
      "loss: 2.211981  [38400/60000]\n",
      "loss: 2.167125  [44800/60000]\n",
      "loss: 2.160066  [51200/60000]\n",
      "loss: 2.118974  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 2.124165 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.134717  [    0/60000]\n",
      "loss: 2.131097  [ 6400/60000]\n",
      "loss: 2.058769  [12800/60000]\n",
      "loss: 2.078204  [19200/60000]\n",
      "loss: 2.015528  [25600/60000]\n",
      "loss: 1.949178  [32000/60000]\n",
      "loss: 1.983135  [38400/60000]\n",
      "loss: 1.890354  [44800/60000]\n",
      "loss: 1.896444  [51200/60000]\n",
      "loss: 1.812211  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.821964 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.856772  [    0/60000]\n",
      "loss: 1.831879  [ 6400/60000]\n",
      "loss: 1.702061  [12800/60000]\n",
      "loss: 1.750267  [19200/60000]\n",
      "loss: 1.624322  [25600/60000]\n",
      "loss: 1.595428  [32000/60000]\n",
      "loss: 1.610197  [38400/60000]\n",
      "loss: 1.514970  [44800/60000]\n",
      "loss: 1.545882  [51200/60000]\n",
      "loss: 1.431730  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.458544 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.527801  [    0/60000]\n",
      "loss: 1.498864  [ 6400/60000]\n",
      "loss: 1.343647  [12800/60000]\n",
      "loss: 1.423108  [19200/60000]\n",
      "loss: 1.292432  [25600/60000]\n",
      "loss: 1.311563  [32000/60000]\n",
      "loss: 1.316470  [38400/60000]\n",
      "loss: 1.246273  [44800/60000]\n",
      "loss: 1.288140  [51200/60000]\n",
      "loss: 1.177590  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.211021 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.287186  [    0/60000]\n",
      "loss: 1.276342  [ 6400/60000]\n",
      "loss: 1.102954  [12800/60000]\n",
      "loss: 1.216915  [19200/60000]\n",
      "loss: 1.084856  [25600/60000]\n",
      "loss: 1.126163  [32000/60000]\n",
      "loss: 1.143000  [38400/60000]\n",
      "loss: 1.081162  [44800/60000]\n",
      "loss: 1.128895  [51200/60000]\n",
      "loss: 1.028185  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.058441 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.126353  [    0/60000]\n",
      "loss: 1.137580  [ 6400/60000]\n",
      "loss: 0.944914  [12800/60000]\n",
      "loss: 1.087882  [19200/60000]\n",
      "loss: 0.960137  [25600/60000]\n",
      "loss: 1.001766  [32000/60000]\n",
      "loss: 1.037635  [38400/60000]\n",
      "loss: 0.977112  [44800/60000]\n",
      "loss: 1.025907  [51200/60000]\n",
      "loss: 0.934160  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.960100 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.014089  [    0/60000]\n",
      "loss: 1.047348  [ 6400/60000]\n",
      "loss: 0.837022  [12800/60000]\n",
      "loss: 1.001718  [19200/60000]\n",
      "loss: 0.882893  [25600/60000]\n",
      "loss: 0.914133  [32000/60000]\n",
      "loss: 0.969490  [38400/60000]\n",
      "loss: 0.909680  [44800/60000]\n",
      "loss: 0.954527  [51200/60000]\n",
      "loss: 0.870638  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.892724 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.931382  [    0/60000]\n",
      "loss: 0.983898  [ 6400/60000]\n",
      "loss: 0.759840  [12800/60000]\n",
      "loss: 0.940552  [19200/60000]\n",
      "loss: 0.832036  [25600/60000]\n",
      "loss: 0.849713  [32000/60000]\n",
      "loss: 0.921881  [38400/60000]\n",
      "loss: 0.864265  [44800/60000]\n",
      "loss: 0.902590  [51200/60000]\n",
      "loss: 0.824581  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.843777 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.867599  [    0/60000]\n",
      "loss: 0.935951  [ 6400/60000]\n",
      "loss: 0.702271  [12800/60000]\n",
      "loss: 0.895099  [19200/60000]\n",
      "loss: 0.795820  [25600/60000]\n",
      "loss: 0.801344  [32000/60000]\n",
      "loss: 0.885815  [38400/60000]\n",
      "loss: 0.832389  [44800/60000]\n",
      "loss: 0.862818  [51200/60000]\n",
      "loss: 0.789492  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.806435 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.816543  [    0/60000]\n",
      "loss: 0.897394  [ 6400/60000]\n",
      "loss: 0.657558  [12800/60000]\n",
      "loss: 0.859947  [19200/60000]\n",
      "loss: 0.768192  [25600/60000]\n",
      "loss: 0.764411  [32000/60000]\n",
      "loss: 0.856391  [38400/60000]\n",
      "loss: 0.808909  [44800/60000]\n",
      "loss: 0.831429  [51200/60000]\n",
      "loss: 0.761118  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.776582 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#We initialize the loss function and optimizer, \n",
    "# and pass it to train_loop and test_loop. \n",
    "# can increase the number of epochs for the model’s improving performance.\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
